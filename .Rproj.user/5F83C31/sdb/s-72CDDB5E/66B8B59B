{
    "contents" : "---\ntitle: \"Milestone Report - Capstone Project\"\nauthor: \"Maurizio Murino\"\ndate: \"21 April 2016\"\noutput: html_document\n---\n\n```{r setoption, cache=TRUE, warning=FALSE, message=FALSE, fig.width=12}\nknitr::opts_chunk$set(cache=TRUE, warning=FALSE, message=FALSE, fig.width=6)\n```\n\n# Introduction\n\nThe goal of this project is just to display that you've gotten used to working with the data and that you are on track to create your prediction algorithm. Please submit a report on R Pubs <http://rpubs.com/> that explains your exploratory analysis and your goals for the eventual app and algorithm. This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. You should make use of tables and plots to illustrate important summaries of the data set. The motivation for this project is to: \n\n1. Demonstrate that you've downloaded the data and have successfully loaded it in;\n2. Create a basic report of summary statistics about the data sets;\n3. Report any interesting findings that you amassed so far;\n4. Get feedback on your plans for creating a prediction algorithm and Shiny app.\n\n\n# Index\n\n0. [Operative questions](# Introduction)\n1. [Loading data](# Loading data)\n2. [Preprocessing](2. Preprocessing)\n3. [Exploratory analysis](3. Exploratory analysis)\n+ [3.1 Most frequent words](3.1 Question 1)\n+ [3.2 Digrams and trigrams](3.2 Digrams and trigrams - Question 2)\n4. [Findings and next steps](4. Findings and next steps)\n\n\n\nText mining refers to the process of parsing a selection or corpus of text in order to identify certain aspects, such as the most frequently occurring word or phrase. \n\n# 1. Loading data\n\nHence, let's download the data from the website and load them into R.\n\n```{r dowload data, eval=FALSE}\ntemp <- tempfile()\ndownload.file(\"https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip\", temp)\n```\n\n```{r loading data, cache=TRUE}\n# List zipped files\nlist.files <- unzip(\"C:/users/MaurizioLocale/downloads/temp.zip\",list=TRUE)\nlist.files\n\n# Extract specific files\nunzip(\"C:/users/MaurizioLocale/downloads/temp.zip\", list.files$Name[c(11:13)], exdir = \"temp\")\n\n# Function to move files \"from\" \"to\" and remove the origianl in \"to\".\n\nmy.file.rename <- function(from, to) {\n    todir <- dirname(to)\n    if (!isTRUE(file.info(todir)$isdir)) dir.create(todir, recursive=TRUE)\n    file.rename(from = from,  to = to)\n}\n\n\n```\n\nNow, it is time to load the R package for text mining and load your texts into R. Furthermore, since the data are too big for my pc, I will sample few lines from the data.\n\n```{r tm, message=FALSE}\nlibrary(tm)   \nlibrary(knitr)\nlibrary(LaF)\nlibrary(SnowballC)\nlibrary(RWeka)\nlibrary(wordcloud)\nlibrary(lattice)\n```\n\n```{r libraries and data memory load, cache=TRUE}\n\nlblog <- determine_nlines(\"./temp/final/en_US/en_US.blogs.txt\")\nltwit <- determine_nlines(\"./temp/final/en_US/en_US.twitter.txt\")\nlnews <- determine_nlines(\"./temp/final/en_US/en_US.news.txt\")\n\nset.seed(123)\nwriteLines(sample_lines(\"./temp/final/en_US/en_US.blogs.txt\", lblog*.05, lblog), \"blogs2.txt\")\nwriteLines(sample_lines(\"./temp/final/en_US/en_US.twitter.txt\", ltwit*.05, ltwit), \"twitter2.txt\")\nwriteLines(sample_lines(\"./temp/final/en_US/en_US.news.txt\", lnews*.05, lnews), \"news2.txt\")\n\nmy.file.rename(\"blogs2.txt\", \"./text2/blogs2.txt\")\nmy.file.rename(\"twitter2.txt\", \"./text2/twitter2.txt\")\nmy.file.rename(\"news2.txt\", \"./text2/news2.txt\")\n\n#Remove complete files\nunlink(\"temp\", recursive = TRUE)\n\ndocs <- Corpus(DirSource(\"./text2\"))  \n\nsummary(docs)  \ninspect(docs)\n```\n\n\n# 2. Preprocessing\n\nOnce the data are loaded properly, let's begin to preprocess the texts. We are going to remove remove numbers, capitalization, common words, punctuation, and otherwise prepare your texts for analysis.\nThis can be somewhat time consuming and picky, but it pays off in the end in terms of high quality analyses. We will start with only 1-grams. We’ll want to pre-process the text before setting up a Term-Document Matrix (TDM) for our word clouds.\n\nWe’ll use tm_map to transform everything to lower case to avoid missing matching between differently cased terms. We’ll also want to remove common junk words (“stopwords”) and custom-defined words of non-interest, remove extra spaces, and take the stems of words to better match recurrence of the root words.\n\n```{r preprocessing}\n# wordlist(c(\"a\", ..., \"n\"))\n#wordlist <- gsub(\"[^[:alnum:]]\", \"\", docs)\n\ndocs <- tm_map(docs, content_transformer(tolower))\n\ndocs <- tm_map(docs, content_transformer(removeWords), \"year\")\n\ndocs <- tm_map(docs, content_transformer(removeWords), stopwords(\"SMART\"))\ndocs <- tm_map(docs, stripWhitespace)\ndocs <- tm_map(docs, removePunctuation)\ndocs <- tm_map(docs, stemDocument)\ndocs <- tm_map(docs, PlainTextDocument)\n```\n\n```{r matrices, cache=TRUE}\ntdm <- TermDocumentMatrix(docs)\ndtm <- DocumentTermMatrix(docs)\n```\n\n# 3. Exploratory analysis\n\n## 3.1 Most frequent words\n\nSome words are more frequent than others - what are the distributions of word frequencies? We will answer this question by, first, produce the termo document matrix and the document term matrix.\n\n```{r freq onegram}\n\ntdm2 <- removeSparseTerms(tdm, 0.85)\ntdm2\n\ndtm2 <- removeSparseTerms(dtm, 0.85)\ndtm2\n```\n\nCreate a WordCloud of our one-grams to visualize the most occurent 100 words.\n\n```{r onegram wordcloud}\nnotsparse <- tdm2\nm = as.matrix(notsparse)\nv = sort(rowSums(m),decreasing=TRUE)\nd = data.frame(word = names(v),freq=v)\n \n# Create the word cloud\npal = brewer.pal(9,\"BuPu\")\nwordcloud(words = d$word,\n          freq = d$freq,\n          scale = c(3,.8),\n          random.order = F,\n          colors = pal,\n          max.words = 50)\n\n```\n\nAnd here we have the frequency table for the most frequent one-grams. To produce the table, we have to produce\n\n```{r frequency for one-gram}\n\nfreq <- sort(colSums(as.matrix(dtm2)), decreasing = TRUE)\nfreqDf <- data.frame(word=names(freq), freq=freq)   \nhead(freqDf)\n\nbarchart(word ~ freq, head(freqDf, 25))\n```\n\n## 3.2 Digrams and trigrams\n\nLet's tokenize our data! We are interested in two-grams.\n\n```{r di grams}\ndiTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))\n\ndiTdm <- TermDocumentMatrix(docs, control = list(tokenize = diTokenizer))\ndiDtm <- DocumentTermMatrix(docs, control = list(tokenize = diTokenizer))\n\ndiTdm2 <- removeSparseTerms(diTdm, 0.90)\ndiTdm2\n\ndiDtm2 <- removeSparseTerms(diDtm, 0.90)\ndiDtm2\n```\n\nCreate a WordCloud to Visualize digrams, because they love wordclouds too!\n\n```{r digrams wordcloud}\nm2 = as.matrix(diTdm2)\nv2 = sort(rowSums(m2),decreasing=TRUE)\nd2 = data.frame(word = names(v2),freq=v2)\nhead(d2)\n \n# Create the word cloud\npal = brewer.pal(9,\"BuPu\")\nwordcloud(words = d2$word,\n          freq = d2$freq,\n          scale = c(2,.7),\n          random.order = F,\n          colors = pal, \n          max.words = 50)\n\n```\n\nThen, the plot representing the more frequent digrams.\n\n```{r frequency for two-gram}\n\nfreq2 <- sort(colSums(as.matrix(diDtm2)), decreasing = TRUE)\nfreqDf2 <- data.frame(word=names(freq2), freq=freq2)   \nhead(freqDf2)\n\nbarchart(word ~ freq, head(freqDf2, 25))\n```\n\nNow, let's repeat the operation for the most interesting trigrams.\n\n```{r tri grams}\ntriTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))\n\ntriTdm <- TermDocumentMatrix(docs, control = list(tokenize = triTokenizer))\ntriDtm <- DocumentTermMatrix(docs, control = list(tokenize = triTokenizer))\n\ntriTdm2 <- removeSparseTerms(triTdm, 0.90)\ntriTdm2\n\ntriDtm2 <- removeSparseTerms(triDtm, 0.90)\ntriDtm2\n```\n\nCreate a WordCloud to Visualize trigrams, because they love wordclouds too!\n\n```{r trigrams wordcloud}\nm3 = as.matrix(triTdm2)\nv3 = sort(rowSums(m3), decreasing = TRUE)\nd3 = data.frame(word = names(v3),freq=v3)\nhead(d3)\n\n# Create the word cloud\npal = brewer.pal(9,\"BuPu\")\nwordcloud(words = d3$word,\n          freq = d3$freq,\n          scale = c(2,.7),\n          random.order = FALSE,\n          colors = pal,\n          max.words = 50)\n\n```\n\nThen, the plot representing the more frequent trigrams.\n\n```{r frequency for tri-gram}\n\nfreq3 <- sort(colSums(as.matrix(triDtm2)), decreasing = TRUE)\nfreqDf3 <- data.frame(word=names(freq3), freq=freq3)   \n\nbarchart(word ~ freq, head(freqDf3, 25))\n\n```\n\n# 4. Findings and next steps\n\nThose task are quite computational demanding for my pc, hence I sampled the data keeping the 5% of the text. In the sample, the concept expressed apparently rotate around **time** and relevant recurrent moments.\n\nNow, I will have to try to develop an algorithm. Still have no a main preference, but it will be based on the data sampled. Cloud computing is not an option, unfortunately.\n\nMost likely, the algorithm will investingate decreasing n-grams until a match is found.\n\nThe Shiny app will be really simple. \n\nThe user interface will consist of a text input box that will allow a user to enter a phrase. Then the app will use our algorithm to suggest the most likely next word.",
    "created" : 1465837920443.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2147082397",
    "id" : "66B8B59B",
    "lastKnownWriteTime" : 1462546298,
    "path" : "C:/Users/MaurizioLocale/OneDrive/Data_Science/10_Capstone_project/CP_Natural_Language/tmTest1.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_markdown"
}